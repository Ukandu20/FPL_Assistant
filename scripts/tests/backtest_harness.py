#!/usr/bin/env python3
r"""
Backtest Harness — leak‑free, GW‑rolling evaluator for MIN / GA / DEF / SAV / xPts

Goals
-----
• Evaluate predictions produced by your forecast scripts under **as‑of GW** windows.
• Enforce **no time travel**: only compare a forecast to gameweeks that were **strictly in the future** at the `as_of_gw` used to generate it.
• Produce **reproducible artifacts**: CSV metrics, calibration tables, per‑GW breakdowns, and optional plots.
• Uniform CLI & output layout mirroring your repo style.

Inputs (predictions already generated by your forecasters)
---------------------------------------------------------
We expect files like these (configurable):
  minutes:  <minutes_root>/<season>/GW{from}_{to}.(csv|parquet)
  ga:       <ga_root>/<season>/GW{from}_{to}.(csv|parquet)
  defense:  <defense_root>/<season>/GW{from}_{to}.(csv|parquet)
  saves:    <saves_root>/<season>/GW{from}_{to}.(csv|parquet)
  points:   <points_root>/<season>/GW{from}_{to}.(csv|parquet)

Labels (truths)
---------------
Provide a labels directory with per‑GW actuals (flexible layout). Minimum columns per task:
  minutes: season, gw, player_id, minutes
  ga:      season, gw, player_id, goals, assists, return_any   # return_any = (goals+assists) > 0
  defense: season, gw, team_id, conceded
  saves:   season, gw, player_id, saves
  points:  season, gw, player_id, total_points

You can give one **wide** file per task or a single combined file. Use --labels-* flags to point to each.

Outputs
-------
  <out_dir>/<season>/
    metrics_summary.csv                 # high‑level metrics across GWs
    by_gw_metrics.csv                   # per‑GW metric table
    calibration_ga_bins.csv             # (if GA evaluated)
    artifacts/                          # any aux files (skips list, coverage, etc.)

CLI Examples
------------
python -m scripts.eval.backtest_harness \
  --seasons 2024-2025,2025-2026 \
  --targets ga,minutes,points \
  --ga-root data/predictions/goals_assists \
  --minutes-root data/predictions/minutes \
  --points-root data/predictions/expected_points \
  --labels-ga data/labels/ga_truth.parquet \
  --labels-minutes data/labels/minutes_truth.parquet \
  --labels-points data/labels/points_truth.parquet \
  --gw-from 1 --gw-to 38 \
  --strict-asof 1 \
  --out-dir data/eval/backtests \
  --out-format both

Notes
-----
• This harness assumes your forecast files encode the **as‑of GW window** in their filename (GWxx_yy). The
  harness treats all rows in GW{from}_{to}.csv as predictions that were made **as of `from`**, for gameweeks in
  [from, to]. Rows with gw < from are ignored; rows with gw > to are also ignored.
• If your forecasters additionally write an `as_of_gw` column, we use that as the source of truth.
• The harness is conservative: if joins fail (ids/gws mismatches), rows are skipped and logged.
"""
from __future__ import annotations

import argparse
import logging
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

import numpy as np
import pandas as pd

# ----------------------------- CLI -------------------------------------------

def _add_common_args(p: argparse.ArgumentParser) -> None:
    p.add_argument("--seasons", required=True, help="Comma-separated seasons, e.g. 2024-2025,2025-2026")
    p.add_argument("--targets", default="ga,minutes,points", help="Subset of tasks to eval: minutes,ga,defense,saves,points")
    p.add_argument("--gw-from", type=int, default=1)
    p.add_argument("--gw-to", type=int, default=38)
    p.add_argument("--strict-asof", type=int, default=1, help="1=drop any rows that violate no-time-travel; 0=warn only")
    p.add_argument("--out-dir", required=True)
    p.add_argument("--out-format", choices=["csv","parquet","both"], default="both")
    p.add_argument("--log-level", default="INFO")

    # Prediction roots
    p.add_argument("--minutes-root")
    p.add_argument("--ga-root")
    p.add_argument("--defense-root")
    p.add_argument("--saves-root")
    p.add_argument("--points-root")

    # Labels
    p.add_argument("--labels-minutes")
    p.add_argument("--labels-ga")
    p.add_argument("--labels-defense")
    p.add_argument("--labels-saves")
    p.add_argument("--labels-points")

    # Filenames
    p.add_argument("--zero-pad-filenames", type=int, default=1, help="Pad GW numbers in filenames (e.g., GW05_07)")

# ----------------------------- IO utils --------------------------------------

def _read_any(path: Path) -> pd.DataFrame:
    if not path.exists():
        raise FileNotFoundError(path)
    if path.suffix == ".parquet":
        return pd.read_parquet(path)
    return pd.read_csv(path)


def _write(df: pd.DataFrame, base: Path, out_format: str) -> None:
    base.parent.mkdir(parents=True, exist_ok=True)
    if out_format in ("csv", "both"):
        df.to_csv(base.with_suffix(".csv"), index=False)
    if out_format in ("parquet", "both"):
        df.to_parquet(base.with_suffix(".parquet"), index=False)


def _gw_fname(gw_from: int, gw_to: int, zpad: bool=True) -> str:
    f = f"{gw_from:02d}" if zpad else str(gw_from)
    t = f"{gw_to:02d}" if zpad else str(gw_to)
    return f"GW{f}_{t}"

# ----------------------------- Metrics ---------------------------------------

def brier_score(p: np.ndarray, y: np.ndarray) -> float:
    return float(np.mean((np.clip(p, 0, 1) - (y > 0).astype(float)) ** 2))


def log_loss(p: np.ndarray, y: np.ndarray, eps: float=1e-9) -> float:
    p = np.clip(p, eps, 1 - eps)
    y = (y > 0).astype(float)
    return float(-np.mean(y * np.log(p) + (1 - y) * np.log(1 - p)))


def mae(yhat: np.ndarray, y: np.ndarray) -> float:
    return float(np.mean(np.abs(yhat - y)))


def rmse(yhat: np.ndarray, y: np.ndarray) -> float:
    return float(np.sqrt(np.mean((yhat - y) ** 2)))


def reliability_bins(p: np.ndarray, y: np.ndarray, n_bins: int=10) -> pd.DataFrame:
    p = np.clip(p, 0, 1)
    y = (y > 0).astype(float)
    bins = np.linspace(0, 1, n_bins + 1)
    idx = np.digitize(p, bins, right=True)
    out = []
    for k in range(1, n_bins + 1):
        mask = idx == k
        if not np.any(mask):
            out.append((k, (bins[k-1]+bins[k])/2, np.nan, 0, np.nan))
        else:
            out.append((k, (bins[k-1]+bins[k])/2, float(p[mask].mean()), int(mask.sum()), float(y[mask].mean())))
    return pd.DataFrame(out, columns=["bin", "bin_center", "mean_pred", "n", "empirical"])

# ----------------------------- Core join logic -------------------------------

PredKey = Tuple[str, int, int]  # (season, as_of_gw, target_gw)


def _iter_pred_files(root: Optional[str], season: str) -> Iterable[Tuple[Path, int, int]]:
    """Yield (path, gw_from, gw_to) for the given season under a predictions root.
    Accept both CSV and Parquet; prefer Parquet when both exist.
    """
    if not root:
        return []
    base = Path(root) / season
    if not base.exists():
        return []
    # gather GWxx_yy base names
    pairs = {}
    for f in base.iterdir():
        name = f.stem if f.suffix != ".parquet" else f.stem
        if name.startswith("GW") and "_" in name:
            try:
                a, b = name.replace("GW", "").split("_")
                gw_from, gw_to = int(a), int(b)
            except Exception:
                continue
            key = (gw_from, gw_to)
            # prefer parquet
            if key not in pairs or f.suffix == ".parquet":
                pairs[key] = f
    for (gf, gt), path in sorted(pairs.items()):
        yield path, gf, gt


def _load_preds_for_window(path: Path, task: str, as_of_gw: int, gw_from: int, gw_to: int) -> pd.DataFrame:
    df = _read_any(path)
    # Optional guard: some forecasters include as_of_gw
    if "as_of_gw" in df.columns:
        df = df[df["as_of_gw"] == as_of_gw]
    # Filter to the target window
    candidate_cols = [c for c in ("gw", "gw_orig", "gw_played") if c in df.columns]
    if not candidate_cols:
        raise ValueError(f"{task}: No gw column in {path}")
    gw_col = candidate_cols[0]
    df = df[(df[gw_col] >= gw_from) & (df[gw_col] <= gw_to)].copy()
    df.rename(columns={gw_col: "gw"}, inplace=True)
    # Normalize id columns
    if task in ("minutes", "ga", "saves", "points"):
        if "player_id" not in df.columns:
            raise ValueError(f"{task}: missing player_id in {path}")
    elif task == "defense":
        if "team_id" not in df.columns:
            raise ValueError(f"defense: missing team_id in {path}")
    return df

# ----------------------------- Task evaluators -------------------------------

def eval_minutes(preds: pd.DataFrame, labels: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    req = ["season","gw","player_id","minutes"]
    for c in req:
        if c not in labels.columns:
            raise ValueError(f"minutes labels missing {c}")
    cols = ["season","gw","player_id","pred_minutes"]
    if "pred_minutes" not in preds.columns and "minutes" in preds.columns:
        preds = preds.rename(columns={"minutes":"pred_minutes"})
    pred = preds[cols].copy()
    df = pred.merge(labels[req], on=["season","gw","player_id"], how="inner", validate="many_to_one")
    if df.empty:
        return pd.DataFrame(), pd.DataFrame()
    df["ae"] = df["pred_minutes"] - df["minutes"]
    by_gw = (df.groupby(["season","gw"], as_index=False)
               .agg(n=("player_id","count"), MAE=("ae", lambda s: float(np.mean(np.abs(s)))), RMSE=("ae", lambda s: float(np.sqrt(np.mean(s**2))))) )
    summary = (by_gw.groupby(["season"], as_index=False)
                 .agg(n_gw=("gw","count"), n_rows=("n","sum"), MAE=("MAE","mean"), RMSE=("RMSE","mean")))
    return summary, by_gw


def eval_ga(preds: pd.DataFrame, labels: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:
    req = ["season","gw","player_id","goals","assists","return_any"]
    for c in req:
        if c not in labels.columns:
            raise ValueError(f"ga labels missing {c}")
    # Standardize prob & mean columns
    proba_col = "p_return_any" if "p_return_any" in preds.columns else None
    mean_col  = "exp_ga" if "exp_ga" in preds.columns else None
    if proba_col is None and mean_col is None:
        raise ValueError("ga preds must include p_return_any and/or exp_ga")

    me = preds[[c for c in ["season","gw","player_id", proba_col, mean_col] if c is not None]].copy()
    df = me.merge(labels[req], on=["season","gw","player_id"], how="inner", validate="many_to_one")
    if df.empty:
        return pd.DataFrame(), pd.DataFrame(), None

    parts = []
    if proba_col is not None:
        parts.append({
            "metric":"brier", "value": brier_score(df[proba_col].to_numpy(), df["return_any"].to_numpy())
        })
        parts.append({
            "metric":"logloss", "value": log_loss(df[proba_col].to_numpy(), df["return_any"].to_numpy())
        })
    if mean_col is not None:
        true_ga = (df["goals"].to_numpy() + df["assists"].to_numpy()).astype(float)
        parts.append({"metric":"MAE","value": mae(df[mean_col].to_numpy(), true_ga)})
        parts.append({"metric":"RMSE","value": rmse(df[mean_col].to_numpy(), true_ga)})

    by_gw_rows = []
    for (season, gw), g in df.groupby(["season","gw"], as_index=True):
        row = {"season":season, "gw":gw, "n": int(len(g))}
        if proba_col is not None:
            row.update({
                "brier": brier_score(g[proba_col].to_numpy(), g["return_any"].to_numpy()),
                "logloss": log_loss(g[proba_col].to_numpy(), g["return_any"].to_numpy()),
            })
        if mean_col is not None:
            true_ga = (g["goals"].to_numpy() + g["assists"].to_numpy()).astype(float)
            row.update({
                "MAE": mae(g[mean_col].to_numpy(), true_ga),
                "RMSE": rmse(g[mean_col].to_numpy(), true_ga),
            })
        by_gw_rows.append(row)
    by_gw = pd.DataFrame(by_gw_rows).sort_values(["season","gw"]).reset_index(drop=True)

    calib = None
    if proba_col is not None:
        calib = reliability_bins(df[proba_col].to_numpy(), df["return_any"].to_numpy(), n_bins=10)
        calib.insert(0, "season", "mixed")

    summary = pd.DataFrame(parts)
    summary.insert(0, "season", "mixed")
    return summary, by_gw, calib


def eval_points(preds: pd.DataFrame, labels: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:
    req = ["season","gw","player_id","total_points"]
    for c in req:
        if c not in labels.columns:
            raise ValueError(f"points labels missing {c}")
    # Normalize prediction column name to xPts
    if "xPts" not in preds.columns:
        alt = [c for c in ("exp_mean","expected_points","xp","pred_points") if c in preds.columns]
        if not alt:
            raise ValueError("points preds require xPts (or a known alias)")
        preds = preds.rename(columns={alt[0]:"xPts"})

    cols = ["season","gw","player_id","xPts"]
    me = preds[cols].copy()
    df = me.merge(labels[req], on=["season","gw","player_id"], how="inner", validate="many_to_one")
    if df.empty:
        return pd.DataFrame(), pd.DataFrame()

    df["ae"] = df["xPts"] - df["total_points"]
    by_gw = (df.groupby(["season","gw"], as_index=False)
               .agg(n=("player_id","count"), MAE=("ae", lambda s: float(np.mean(np.abs(s)))), RMSE=("ae", lambda s: float(np.sqrt(np.mean(s**2))))) )
    summary = (by_gw.groupby(["season"], as_index=False)
                 .agg(n_gw=("gw","count"), n_rows=("n","sum"), MAE=("MAE","mean"), RMSE=("RMSE","mean")))
    return summary, by_gw

# ----------------------------- Label loading ---------------------------------

def _load_labels(path: Optional[str]) -> pd.DataFrame:
    if not path:
        return pd.DataFrame()
    return _read_any(Path(path))

# ----------------------------- Orchestration ---------------------------------

def run(args: argparse.Namespace) -> None:
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), logging.INFO), format="[%(levelname)s] %(message)s")

    seasons = [s.strip() for s in args.seasons.split(",") if s.strip()]
    targets = [t.strip() for t in args.targets.split(",") if t.strip()]

    label_map = {
        "minutes": _load_labels(args.labels_minutes),
        "ga": _load_labels(args.labels_ga),
        "defense": _load_labels(args.labels_defense),
        "saves": _load_labels(args.labels_saves),
        "points": _load_labels(args.labels_points),
    }

    root_map = {
        "minutes": args.minutes_root,
        "ga": args.ga_root,
        "defense": args.defense_root,
        "saves": args.saves_root,
        "points": args.points_root,
    }

    overall_metrics: List[pd.DataFrame] = []
    all_by_gw: List[pd.DataFrame] = []
    calib_bins: List[pd.DataFrame] = []

    zpad = bool(args.zero_pad_filenames)

    for season in seasons:
        out_season = Path(args.out_dir) / season
        (out_season / "artifacts").mkdir(parents=True, exist_ok=True)

        # Collect preds across all windows for this season per task
        for task in targets:
            root = root_map.get(task)
            if not root:
                logging.warning(f"[{task}] No predictions root provided; skipping")
                continue

            label_df = label_map.get(task, pd.DataFrame())
            if label_df.empty:
                logging.warning(f"[{task}] Labels not provided or empty; skipping")
                continue

            # Only keep label rows within GW range
            label_df = label_df[(label_df["season"] == season) & (label_df["gw"] >= args.gw_from) & (label_df["gw"] <= args.gw_to)].copy()
            if label_df.empty:
                logging.warning(f"[{task}] No label rows in range for {season}")
                continue

            # Stitch all relevant prediction windows
            stitched: List[pd.DataFrame] = []
            coverage_rows = []
            for path, gf, gt in _iter_pred_files(root, season):
                # Enforce window intersection with [gw_from, gw_to]
                if gt < args.gw_from or gf > args.gw_to:
                    continue
                as_of = gf
                # Predictions are for target gws in [gf, gt]
                target_from = max(gf, args.gw_from)
                target_to   = min(gt, args.gw_to)
                try:
                    df = _load_preds_for_window(path, task, as_of_gw=as_of, gw_from=target_from, gw_to=target_to)
                except Exception as e:
                    logging.error(f"[{task}] Failed reading {path}: {e}")
                    continue

                # If strict_asof, ensure gw >= as_of
                if args.strict_asof:
                    before = len(df)
                    df = df[df["gw"] >= as_of].copy()
                    dropped = before - len(df)
                    if dropped:
                        logging.info(f"[{task}] Dropped {dropped} rows violating as_of gw in {path.name}")

                df["season"] = season
                df["as_of_gw"] = as_of
                stitched.append(df)
                coverage_rows.append({"file": path.name, "as_of_gw": as_of, "gw_from": target_from, "gw_to": target_to, "rows": len(df)})

            if not stitched:
                logging.warning(f"[{task}] No prediction files found for {season} within GW range")
                continue

            preds = pd.concat(stitched, ignore_index=True)
            coverage = pd.DataFrame(coverage_rows)
            coverage.to_csv(out_season / "artifacts" / f"coverage_{task}.csv", index=False)

            # Evaluate per task
            try:
                if task == "minutes":
                    summ, by_gw = eval_minutes(preds, label_df)
                elif task == "ga":
                    summ, by_gw, calib = eval_ga(preds, label_df)
                    if calib is not None:
                        calib["task"] = task
                        calib["season"] = season
                        calib_bins.append(calib)
                elif task == "points":
                    summ, by_gw = eval_points(preds, label_df)
                else:
                    logging.warning(f"[{task}] Evaluator not implemented yet; skipping")
                    continue
            except Exception as e:
                logging.error(f"[{task}] Evaluation failed: {e}")
                continue

            if not summ.empty:
                summ.insert(0, "task", task)
                summ.loc[:, "season"] = season
                overall_metrics.append(summ)
            if not by_gw.empty:
                by_gw.insert(0, "task", task)
                by_gw.loc[:, "season"] = season
                all_by_gw.append(by_gw)

        # Write season artifacts
        if overall_metrics:
            om = pd.concat([df for df in overall_metrics if df["season"].iloc[0] == season], ignore_index=True)
            _write(om, out_season / "metrics_summary", args.out_format)
        if all_by_gw:
            bgw = pd.concat([df for df in all_by_gw if df["season"].iloc[0] == season], ignore_index=True)
            _write(bgw, out_season / "by_gw_metrics", args.out_format)
        if calib_bins:
            cb = pd.concat([df for df in calib_bins if df["season"].iloc[0] == season], ignore_index=True)
            _write(cb, out_season / "calibration_ga_bins", args.out_format)

    logging.info("Backtest complete.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GW‑rolling backtest harness for FPL predictions")
    _add_common_args(parser)
    args = parser.parse_args()
    run(args)
